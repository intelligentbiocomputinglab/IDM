{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 2)\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "import mdshare\n",
    "\n",
    "### Note: Please replace the working_directory with the current one ! ###\n",
    "wd = '/idiom/demo/' ### This defines the working directory\n",
    "\n",
    "local_filename = mdshare.fetch('alanine-dipeptide-3x250ns-backbone-dihedrals.npz',working_directory=wd )\n",
    "with np.load(local_filename) as fh:\n",
    "    trajs = [fh[key] for key in fh.keys()]\n",
    "    \n",
    "local_filename1 = mdshare.fetch('alanine-dipeptide-3x250ns-heavy-atom-positions.npz',working_directory=wd )\n",
    "with np.load(local_filename1) as fh:\n",
    "    trajs1 = [fh[key] for key in fh.keys()]\n",
    "    \n",
    "traj_concat = np.concatenate((trajs[0],trajs[1],trajs[2]),axis=0)[::1,:]\n",
    "print(traj_concat.shape)\n",
    "data_size,_ = traj_concat.shape\n",
    "# print(traj_concat[:10,:])\n",
    "\n",
    "data_phi = traj_concat[:,0]\n",
    "data_psi = traj_concat[:,1]\n",
    "torsion = np.column_stack((data_phi,data_psi))\n",
    "\n",
    "########## xyz:\n",
    "traj_concat1 = np.concatenate((trajs1[0],trajs1[1],trajs1[2]),axis=0)[::1,:]\n",
    "\n",
    "atom_position_list = np.split(traj_concat1,10,axis=1)\n",
    "# print(atom_position_list[1][0])\n",
    "\n",
    "dist_list = []\n",
    "for i in range(10):\n",
    "    for j in range(i+1,10):\n",
    "        dist_tmp = np.sum(\n",
    "            np.square(atom_position_list[i]-atom_position_list[j]),\n",
    "        axis=-1)\n",
    "        dist_list.append(dist_tmp)\n",
    "        \n",
    "print(len(dist_list))\n",
    "\n",
    "dist_array_raw = np.column_stack(dist_list)\n",
    "dist_array = (dist_array_raw - np.mean(dist_array_raw,axis=0,keepdims=True))/( np.std(dist_array_raw,axis=0,keepdims=True) ) ## Batch Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task Sampling by FPS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distance(A, B):\n",
    "            return np.sqrt( np.sum( ( A - B )**2 , axis=-1) ) ### [N,s] or [N]\n",
    "        \n",
    "def MaxMinSampling(points, seed_id, n=1000):\n",
    "    \n",
    "    seed_id = seed_id.astype(np.int32)\n",
    "    seed = points[ seed_id ]\n",
    "    s,_ = seed.shape\n",
    "    \n",
    "    landmark = seed_id.tolist() ### [s], to be expanded\n",
    "    \n",
    "    pts_3d = np.expand_dims(points,axis=1) ## [N,1,dim]\n",
    "    seed_3d = np.expand_dims(seed,axis=0) ## [1,s,dim]\n",
    "    \n",
    "    m = np.min( distance(pts_3d,seed_3d), axis=-1) ### [N]\n",
    "    \n",
    "    for i in range(s,n):\n",
    "        lm = np.argmax(m)\n",
    "        landmark.append( lm ) \n",
    "        ###\n",
    "        dist = distance( np.expand_dims(points[lm],axis=0) , points ) ### [N]\n",
    "        m = np.minimum( m, dist )\n",
    "        \n",
    "    return landmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform mini-batch Farthest Point Sampling (FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_id_list = []\n",
    "\n",
    "n_iter = 10\n",
    "minibs = data_size // n_iter\n",
    "id_set = np.arange(minibs)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print('Iteration: ', i)\n",
    "    np.random.shuffle(id_set)\n",
    "    seed_id = id_set[:100]\n",
    "    \n",
    "    dist_array_tmp = dist_array[ i*minibs : (i+1)*minibs ]\n",
    "    win_id = MaxMinSampling( dist_array_tmp, seed_id=seed_id, n=1000 ) ###minibs\n",
    "    win_id = np.array(win_id).astype(np.int32)\n",
    "    \n",
    "    win_id_shifted = i*minibs + win_id\n",
    "    win_id_list.append(win_id_shifted)\n",
    "    \n",
    "id_array = np.concatenate(win_id_list)\n",
    "np.savetxt('./fps.txt', id_array .reshape((-1,1)),fmt='%d')\n",
    "np.savetxt('./torsion.fps.txt',torsion[id_array])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
